{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image captioning deep learning workshop.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamirdh/Deep-learning-image-caption/blob/master/image_captioning_deep_learning_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ_DSjOA83Oe"
      },
      "source": [
        "# Downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkLieRxMxqdZ"
      },
      "source": [
        "%cd /content\n",
        "!rm -rf coco\n",
        "!rm -rf sample_data\n",
        "!mkdir coco\n",
        "%cd coco\n",
        "!mkdir images\n",
        "%cd images\n",
        "!mkdir train2017\n",
        "\n",
        "#!wget -c http://images.cocodataset.org/zips/train2017.zip\n",
        "!wget -c http://images.cocodataset.org/zips/val2017.zip\n",
        "\n",
        "#!unzip train2017.zip\n",
        "!unzip val2017.zip\n",
        "\n",
        "#!rm train2017.zip\n",
        "!rm val2017.zip\n",
        "\n",
        "%cd ../\n",
        "!wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "\n",
        "!unzip annotations_trainval2017.zip\n",
        "\n",
        "!rm annotations_trainval2017.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEZMYzlIxCwV"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnc6rR2P-ck-"
      },
      "source": [
        "## Imports and Vocabulary "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YUFskmKSJlD"
      },
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from pycocotools.coco import COCO\n",
        "from PIL import Image\n",
        "import pickle\n",
        "\n",
        "spacy_eng = spacy.load(\"en\")\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self,freq_threshold):\n",
        "        #setting the pre-reserved tokens int to string tokens\n",
        "        # PAD- padding symbol\n",
        "        # SOS- Start of Sentence\n",
        "        # EOS- end of sentence\n",
        "        # UNK- unknown word (unknown\\ below threshold)\n",
        "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
        "        #string to int tokens\n",
        "        #its reverse dict self.itos\n",
        "        self.stoi = {v:k for k,v in self.itos.items()}\n",
        "        self.freq_threshold = freq_threshold\n",
        "        \n",
        "    def __len__(self):\n",
        "      return len(self.itos)\n",
        "    \n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
        "    \n",
        "    def build_vocab(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "        for index,sentence in enumerate(sentence_list):\n",
        "\n",
        "            for word in self.tokenize(sentence):\n",
        "                frequencies[word] += 1\n",
        "                \n",
        "                #add the word to the vocab if it reaches minum frequecy threshold\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    if idx > 0 and idx % 1000==0:\n",
        "                        print(f\"Added {idx} words to vocab\")\n",
        "                    idx += 1\n",
        "            if index>0 and index%1000==0:\n",
        "                print(f\"Iterated {index} sentences\")\n",
        "             \n",
        "\n",
        "        print(f\"Done, added {idx-1} words to vocabulary\")\n",
        "    \n",
        "    def numericalize(self,text):\n",
        "        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n",
        "        tokenized_text = self.tokenize(text)\n",
        "        result = [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkgLLalGAHXn"
      },
      "source": [
        "## Dataset custom class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y19oWTAsWmEC"
      },
      "source": [
        "import pickle\n",
        "class COCODataset(Dataset):\n",
        "    \"\"\"\n",
        "    COCODataset\n",
        "    \"\"\"\n",
        "    def __init__(self,root_dir,annotation_file,transform=None,freq_threshold=5,\n",
        "                 load_vocab=False, vocab_loc = \"vocab.pkl\"):\n",
        "      \"\"\"\n",
        "      can use load_vocab to use a previously created vocabulary (time saving feature)\n",
        "      freq_threshold: words with a count below this number will be marked as <UNK>\n",
        "      \"\"\"\n",
        "      self.root_dir = root_dir\n",
        "      self.coco = COCO(annotation_file)\n",
        "      self.transform = transform\n",
        "      self.cap_max_size = 0\n",
        "      #Get image and caption colum from the dataframe\n",
        "      self.imgs = []\n",
        "      self.captions = []\n",
        "      for idx,ann in enumerate(self.coco.anns.values()):\n",
        "        self.imgs.append(self.coco.loadImgs((ann['image_id']))[0][\"file_name\"])\n",
        "        self.captions.append(ann['caption'])\n",
        "        if (idx) % 1000 == 0 and idx>0:\n",
        "          print(f\"Processed {idx} images and captions\")\n",
        "      print(\"Finished processing images and captions\")\n",
        "      print(f\"Got:{len(set(self.imgs))} pictures with {len(self.captions)} captions!\")\n",
        "      \n",
        "      #Initialize vocabulary and build vocab\n",
        "      if load_vocab:\n",
        "        with open(vocab_loc, \"rb\") as source:\n",
        "          self.vocab = pickle.load(source)\n",
        "        print(f\"Loaded vocabulary from {vocab_loc}\")\n",
        "      \n",
        "      else:\n",
        "        print(\"Build vocabulary\")\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocab(self.captions)\n",
        "        print(\"Finished building vocabulary\")\n",
        "        with open(vocab_loc, \"wb\") as dest:\n",
        "          pickle.dump(self.vocab, dest)\n",
        "      \n",
        "      print(f\"Using {len(self.vocab)} words\")\n",
        "    \n",
        "    def __len__(self):\n",
        "      return len(self.imgs)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "      caption = self.captions[idx]\n",
        "      img_name = self.imgs[idx]\n",
        "      img_location = os.path.join(self.root_dir,img_name)\n",
        "      img = Image.open(img_location).convert(\"RGB\")\n",
        "      \n",
        "      #apply the transfromation to the image\n",
        "      if self.transform:\n",
        "          img = self.transform(img)\n",
        "      \n",
        "      #numericalize the caption text\n",
        "      caption_vec = [self.vocab.stoi[\"<SOS>\"]]\n",
        "      caption_vec.extend(self.vocab.numericalize(caption))\n",
        "      caption_vec.append(self.vocab.stoi[\"<EOS>\"])\n",
        "      \n",
        "      return img, torch.tensor(caption_vec,dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDqCBk-KCBkw"
      },
      "source": [
        "## Dataloader creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNHTCZ8ZcXGQ"
      },
      "source": [
        "# define a transformation to add some noise and variance to our images\n",
        "transformation = transforms.Compose([transforms.Resize((512,512), Image.NEAREST),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.RandomInvert(),\n",
        "                                     transforms.RandomVerticalFlip(),\n",
        "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                                      ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_flS7fBeC0r"
      },
      "source": [
        "class CapsCollate:\n",
        "    \"\"\"\n",
        "    Collate to apply the padding to the captions with dataloader\n",
        "    \"\"\"\n",
        "    def __init__(self,pad_idx,batch_first=False, vec_len=-1):\n",
        "        self.pad_idx = pad_idx\n",
        "        self.batch_first = batch_first\n",
        "        self.vec_len = vec_len + 2 # adding the <SOS> and <EOS>\n",
        "        assert self.vec_len > 0, \"Vector length must be positive integer\"\n",
        "    \n",
        "    def __call__(self,batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs,dim=0)\n",
        "        targets_list = list()\n",
        "        for item in batch:\n",
        "            # item = (img:Image, caption:tensor)\n",
        "            addition = self.vec_len-len(item[1])\n",
        "            padded_target = torch.cat((item[1], torch.empty(addition,dtype=torch.long).fill_(pad_idx)),dim=0)\n",
        "            targets_list.append(padded_target)\n",
        "            #print(f\"GOT:{item[1]}, {item[1].type()}\\nAdding:{addition}\\nPADDED:{padded_target}\\n{padded_target.type()}\")\n",
        "        targets = torch.stack(targets_list,0)\n",
        "        #print(f\"Targets shape:{targets.shape}\")\n",
        "        return imgs,targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igD_-Qs7oFTz"
      },
      "source": [
        "\n",
        "dataset =  COCODataset(\n",
        "    root_dir = \"/content/coco/images/val2017\",\n",
        "    annotation_file= \"/content/coco/annotations/captions_val2017.json\",\n",
        "    transform=transformation,\n",
        "    freq_threshold=5,\n",
        "    load_vocab=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa_fIXj5_n6W"
      },
      "source": [
        "print(dataset.vocab.stoi) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak2Viy6LglyP"
      },
      "source": [
        "# from google.colab import files\n",
        "# files.download('vocab.pkl') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1ptid0sedWA"
      },
      "source": [
        "\n",
        "BATCH_SIZE = 1\n",
        "NUM_WORKER = 2\n",
        "#token to represent the padding\n",
        "pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "data_loader = DataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKER,\n",
        "    shuffle=False,\n",
        "    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True, vec_len=75)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXIFRNd6Rbih"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxeR_Dg0JnNQ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "  def __init__(self, embed_size, train_CNN=False):\n",
        "      super(EncoderCNN, self).__init__()\n",
        "      self.train_CNN = train_CNN\n",
        "      self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
        "      self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, images):\n",
        "      \n",
        "      features = self.inception(images)\n",
        "      output = self.dropout(self.relu(features))\n",
        "      return output\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "  \"\"\"\n",
        "  Input is a CNN network, output will be a caption.\n",
        "  TODO: Check how to implement a transformer for better results\n",
        "  \"\"\"\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "      super(DecoderRNN, self).__init__()\n",
        "      self.hidden_size = hidden_size\n",
        "      self.vocab_size = vocab_size\n",
        "      self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "      self.lstm_cell = nn.LSTMCell(embed_size, hidden_size)\n",
        "      self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "      self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, features, captions, show=False):\n",
        "        # batch size\n",
        "        batch_size = features.size(0)\n",
        "        \n",
        "        \n",
        "        # init the hidden and cell states to zeros\n",
        "        hidden_state = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
        "        cell_state = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
        "        hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))\n",
        "        # define the output tensor placeholder\n",
        "        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).to(device)\n",
        "\n",
        "        # embed the captions\n",
        "        captions_embed = self.embed(captions)\n",
        "        # tensor of shape (B, LEN, EMBED SIZE)\n",
        "        # LEN- vectors length (longest caption+2)\n",
        "        \n",
        "        # pass the caption word by word\n",
        "        for t in range(captions.size(1)):\n",
        "\n",
        "            # for the first time step the input is the feature vector\n",
        "            # if t == 0:\n",
        "            #     hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))\n",
        "                \n",
        "            # # for the 2nd+ time step, using teacher forcer\n",
        "            # else:\n",
        "            hidden_state, cell_state = self.lstm_cell(captions_embed[:, t, :], (hidden_state, cell_state))\n",
        "            # output of the attention mechanism\n",
        "            out = self.fc_out(self.dropout(hidden_state))\n",
        "            # build the output tensor\n",
        "            outputs[:, t, :] = out\n",
        "        if show:\n",
        "            #print(f\"Captions:{captions}\")\n",
        "            #print(f\"outputs shape:{outputs.shape}\")\n",
        "            pass\n",
        "        return outputs\n",
        "\n",
        "  \n",
        "\n",
        "class CNNtoRNN(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, train_CNN=False):\n",
        "      super(CNNtoRNN, self).__init__()\n",
        "      self.encoderCNN = EncoderCNN(embed_size, train_CNN).to(device)\n",
        "      self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
        "      #self.decoderRNN = DecoderRNNConcat(embed_size, hidden_size, vocab_size).to(device)\n",
        "\n",
        "  def forward(self, images, captions, show=False):\n",
        "      features = self.encoderCNN(images)\n",
        "      outputs = self.decoderRNN(features, captions, show)\n",
        "      return outputs\n",
        "  def caption_images(self, image, vocab, max_len=50):\n",
        "      # Inference part\n",
        "      # Given the image features generate the captions\n",
        "      # input shape: (3,x,y) where, x,y: image size\n",
        "      # ouput: captions list\n",
        "    batch_size = image.size(0)\n",
        "    assert batch_size == 1, \"Caption 1 image at a time\"\n",
        "    image_pred = self.encoderCNN(image)\n",
        "\n",
        "    # init the hidden and cell states to zeros\n",
        "    hidden_state = torch.zeros((1, self.decoderRNN.hidden_size)).to(device)\n",
        "    cell_state = torch.zeros((1, self.decoderRNN.hidden_size)).to(device)\n",
        "    \n",
        "    #starting input is \n",
        "    captions = list()\n",
        "    outputs = torch.empty((batch_size, max_len, self.decoderRNN.vocab_size)).to(device)\n",
        "    hidden_state, cell_state = self.decoderRNN.lstm_cell(image_pred, (hidden_state, cell_state))\n",
        "    out = self.decoderRNN.fc_out(self.decoderRNN.dropout(hidden_state))\n",
        "    word_embed = self.decoderRNN.embed(torch.tensor(vocab.stoi[\"<SOS>\"]).to(device)).unsqueeze(0)\n",
        "    for t in range(max_len):\n",
        "        # for the first time step the input is the feature vector\n",
        "        # if t == 0:\n",
        "        #     hidden_state, cell_state = self.decoderRNN.lstm_cell(image_pred, (hidden_state, cell_state))\n",
        "        # for the 2nd+ time step, use previously generated caption\n",
        "        # else:\n",
        "        hidden_state, cell_state = self.decoderRNN.lstm_cell(word_embed, (hidden_state, cell_state))\n",
        "        \n",
        "        # output of the attention mechanism\n",
        "        out = self.decoderRNN.fc_out(hidden_state)\n",
        "        outputs[:, t, :] = out\n",
        "        #print(f\"out shape:{out.shape}\")\n",
        "        captions.append(torch.argmax(out,dim=1))\n",
        "        #print(f\"\\n predicted outputs:{captions}\")\n",
        "        word_embed = self.decoderRNN.embed(torch.argmax(out[0])).unsqueeze(0)\n",
        "        last_word_idx = captions[-1].item()\n",
        "        if vocab.itos[last_word_idx] == vocab.stoi[\"<EOS>\"]:\n",
        "            print(\"BREAKING!!!!!!\")\n",
        "            break\n",
        "            \n",
        "            \n",
        "            # build the output tensor\n",
        "        #print(captions)\n",
        "        #covert the vocab idx to words and return sentence\n",
        "    print(f\"outputs shape:{outputs.shape}\")\n",
        "    print(f\"Outputs argmax dim2:{torch.argmax(outputs,dim=2)}\")\n",
        "    return [vocab.itos[idx.item()] for idx in captions if idx.item() != vocab.stoi[\"<PAD>\"]]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTBnd6f5RgQd"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qTIQN7TWqOJ"
      },
      "source": [
        "## training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu_HfkQyRiAP"
      },
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train(max_epochs, model):\n",
        "  # Hyperparameters\n",
        "  learning_rate = 3e-4\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  \n",
        "  # init model\n",
        "  model = model.to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  model.train()\n",
        "     \n",
        "\n",
        "  # start epochs\n",
        "  for epoch in range(max_epochs):\n",
        "    for idx, (img, captions) in tqdm(\n",
        "            enumerate(data_loader), total=len(data_loader), leave=False\n",
        "        ):\n",
        "      img = img.to(device)\n",
        "      captions = captions.to(device).long()\n",
        "      output = model(img, captions).to(device)\n",
        "      loss = criterion(output.reshape(-1, output.shape[2]), captions.reshape(-1))\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward(loss)\n",
        "      optimizer.step()\n",
        "\n",
        "      if idx>0 and idx%100==0:\n",
        "        dataiter = iter(data_loader)\n",
        "        img_show,cap = next(dataiter)\n",
        "        print(f\"Loss {loss.item():.5f}\\n\")\n",
        "        demo_cap = model.caption_images(img_show[0:1].to(device), vocab=dataset.vocab, max_len=30)\n",
        "        demo_cap = ' '.join(demo_cap)\n",
        "        print(\"Predicted\")\n",
        "        show_image(img_show[0],title=demo_cap)\n",
        "        print(\"Original\")\n",
        "        cap = cap[0]\n",
        "        print(cap.long())\n",
        "        demo_cap = ' '.join([dataset.vocab.itos[idx2.item()] for idx2 in cap if idx2.item() != dataset.vocab.stoi[\"<PAD>\"]])\n",
        "        show_image(img_show[0],title=demo_cap, transform=False)\n",
        "        \n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CWQGhuOWvF9"
      },
      "source": [
        "## image function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1APdbNyWw8R"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def show_image(img, title=None, transform=True):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    \n",
        "    #unnormalize \n",
        "    if transform:\n",
        "      img[0] = img[0] * 0.229\n",
        "      img[1] = img[1] * 0.224 \n",
        "      img[2] = img[2] * 0.225 \n",
        "      img[0] += 0.485 \n",
        "      img[1] += 0.456 \n",
        "      img[2] += 0.406\n",
        "      \n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    \n",
        "    \n",
        "    plt.imshow(img)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KceQyQ0uqiRM"
      },
      "source": [
        "## Overfit sanity check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcQJ733Nql1V"
      },
      "source": [
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "def overfit(model, T=250):\n",
        "    learning_rate = 3e-4\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    \n",
        "    # init model\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    model.train()\n",
        "\n",
        "    \n",
        "    dataiter = iter(data_loader)\n",
        "    img,caption = next(dataiter)\n",
        "    for i in tqdm(range(T)):\n",
        "        # train on the same image and caption to achieve overfitting\n",
        "        img = img.to(device)\n",
        "        caption = caption.to(device).long()\n",
        "        output = model(img, caption, show=False).to(device)\n",
        "        loss = criterion(output.reshape(-1, output.shape[2]), caption.reshape(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(loss)\n",
        "        optimizer.step()\n",
        "    output = model(img, caption, show=False).to(device)\n",
        "    show_img = img.to(\"cpu\")\n",
        "    print(f\"\\n\\nLoss {loss.item():.5f}\\n\")\n",
        "    #print(f\"\\nForward\\n\")\n",
        "    out_cap = torch.argmax(output[0],dim=1)\n",
        "    #print(f\"Forwad num vals:{out_cap}\")\n",
        "    demo_cap = ' '.join([dataset.vocab.itos[idx2.item()] for idx2 in out_cap if idx2.item() != dataset.vocab.stoi[\"<PAD>\"]])\n",
        "    show_image(show_img[0],title=demo_cap)\n",
        "    print(\"Predicted\")\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        demo_cap = model.caption_images(show_img[0:1].to(device), vocab=dataset.vocab, max_len=15)\n",
        "        demo_cap = ' '.join(demo_cap)\n",
        "        model.train()\n",
        "        \n",
        "        show_image(show_img[0],title=demo_cap, transform=False)\n",
        "    print(\"Original\")\n",
        "    cap = caption[0]\n",
        "    #print(cap.long())\n",
        "    demo_cap = ' '.join([dataset.vocab.itos[idx2.item()] for idx2 in cap if idx2.item() != dataset.vocab.stoi[\"<PAD>\"]])\n",
        "    show_image(show_img[0],title=demo_cap, transform=False)\n",
        "\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rABCvI85Wx_9"
      },
      "source": [
        "## Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6aG4SBFV7UM"
      },
      "source": [
        "embed_size = 300\n",
        "hidden_size = 5000\n",
        "vocab_size = len(dataset.vocab)\n",
        "model = CNNtoRNN(embed_size, hidden_size, vocab_size, train_CNN=False)\n",
        "trained_model = train(3, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Eqf6B56tjS1"
      },
      "source": [
        "# Misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQmoP_Ltd9pM"
      },
      "source": [
        "embed_size = 400\n",
        "hidden_size = 3000\n",
        "vocab_size = len(dataset.vocab)\n",
        "model = CNNtoRNN(embed_size, hidden_size, vocab_size, train_CNN=False)\n",
        "overfit(model,250)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtuAGKC3gftz"
      },
      "source": [
        "for imgs,caps in data_loader:\n",
        "    print(f\"\\ncap shape:{caps.shape}\")\n",
        "    print(f\"\\nFirst caption:{caps[0]}\")\n",
        "    print(f\"\\nFirst embed:{caps[0][0]}\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}